{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a43330-16e7-446a-b3bc-c7298d30431b",
   "metadata": {},
   "source": [
    "## Data Science Workshop-2 (CSE 2196)\n",
    "### ASSIGNMENT-5\n",
    "\n",
    "### Name: Anish Kumar\n",
    "### Regd. No: 2241011101\n",
    "### Section: 2241007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487dacfd-f3fe-4841-9ccd-42af014b34ff",
   "metadata": {},
   "source": [
    "### 1. We have seen in linear regression when the cost function is\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (\\theta^T x^{(i)} - y^{(i)})^2 \n",
    "$$\n",
    "### the closed form formula for θ is given by\n",
    "$$\n",
    "\\theta = (X^T X + \\lambda I)^{-1} X^T y\n",
    "$$\n",
    "### Now consider the cost function\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (\\theta^T x^{(i)} - y^{(i)})^2 + \\frac{\\lambda}{2} ||\\theta||^2 = \\frac{1}{2} (X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) + \\frac{\\lambda}{2} ||\\theta||^2$$\n",
    "\n",
    "### and find the closed form expression for θ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba8318-ae72-47ee-810e-f6503206e8f3",
   "metadata": {},
   "source": [
    "The Cost Function: <br>\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (\\theta^T x^{(i)} - y^{(i)})^2 + \\frac{\\lambda}{2} ||\\theta||^2 $$ \n",
    "The Cost Function is regularized regression cost function, also known as Ridge Regression. The reguarlization term \n",
    "$$ \\frac{\\lambda}{2} ||\\theta||^2 $$ \n",
    "is added to prevent overfitting by penalizing large values of the parameters. <br>\n",
    "To find the closed form we will Minimize $$ J(\\theta) $$\n",
    "\n",
    "So, \n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = X^T (X\\theta - \\vec{y}) + \\lambda \\theta = 0\n",
    "$$\n",
    "\n",
    "Expanding, we get:\n",
    "\n",
    "$$\n",
    "X^T X \\theta - X^T \\vec{y} + \\lambda \\theta = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^T X \\theta + \\lambda \\theta = X^T \\vec{y}\n",
    "$$\n",
    "$$\n",
    "(X^T X + \\lambda I) \\theta^{-1} X^T \\vec{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393e900",
   "metadata": {},
   "source": [
    "### 2. Explain Gini impurity with an example.\n",
    "Gini impurity is a metric used to measure the impurity or disorder of a dataset. It is used in decision tree algorithms to determine the best attribute and threshold for splitting the dataset into subsets. The goal is to achieve the purest possible subsets, where most items belong to a single class.\n",
    "\n",
    "$$\n",
    "Gini(p) = 1 - \\sum (p_i)^2\n",
    "$$\n",
    "\n",
    "Example: A dataset of persons categorized into two classes: ‘Male’ and ‘female’. Where Male appear 3 times, and female also appears 3 times.\n",
    "\n",
    "The distribution of labels in this dataset is as follows: \n",
    "- Male: 3 elements out of 6 (50%), female: 3 elements out of 6 (50%)\n",
    "- For Male (p =0.5) and female (p =0.5)\n",
    "- The Gini impurity would be: 1 −(0.5)2 −(0.5)2 = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd04da",
   "metadata": {},
   "source": [
    "### 3. Explain Entropy of a dataset with an example.\n",
    "Entropy is a metric used to measure the impurity or disorder of a dataset. It is used in decision tree algorithms to determine the best attribute and threshold for splitting the dataset into subsets. Higher entropy indicates more impurity, while lower entropy indicates more purity. The goal is to minimize entropy in each split to create the most homogeneous subsets.\n",
    "\n",
    "The formula for entropy is:\n",
    "\n",
    "$$\n",
    "Gini(p) = 1 - [(p(M))^2 + (p(F))^2]\n",
    "$$\n",
    "\n",
    "Example: A dataset of persons categorized into two classes: ‘Male’ and ‘female’. Where Male appear 3 times, and female also appears 3 times.\n",
    "\n",
    "The distribution of labels in this dataset is as follows: \n",
    "- Male: 3 elements out of 6 (50%), female: 3 elements out of 6 (50%)\n",
    "- For Male (p =0.5) and female (p =0.5)\n",
    "- the Entropy would be: 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425ead3-01ab-47ce-a9ab-c95f5a6b90c0",
   "metadata": {},
   "source": [
    "### 4. Is a Node’s Entropy is generally higher than it’s parent’s entropy? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa99612-3f3c-41b5-b0f1-efd87f8e39c2",
   "metadata": {},
   "source": [
    "No, a node’s entropy is generally not higher than its parent’s entropy. In fact, the goal of splitting a node in a decision tree is to decrease entropy, which means making the data more pure.\n",
    "\n",
    "When a node in a decision tree is split, the dataset is divided into two or more subsets. The aim is to organize the data in such a way that each subset contains instances that are as similar as possible to each other (i.e., belong to the same class) and as different as possible from instances in other subsets. This would result in a decrease in entropy because the subsets are more pure than the original set.\n",
    "\n",
    "The decision tree algorithm chooses the splits that result in the largest decrease in entropy. Therefore, child nodes should have lower entropy than their parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211a851-039f-4d6c-9d6e-3e0654d8f77a",
   "metadata": {},
   "source": [
    "### 5. Difference between the followings\n",
    "- Bagging and Boosting \n",
    "- Random Forest and Bagged decision tree\n",
    "- Splitting and pruning\n",
    "- Hard Voting and soft voting\n",
    "- K Means clustering and K nearest neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9fe19-b0e3-4bda-b24a-a76fd681be73",
   "metadata": {},
   "source": [
    "  <b> &emsp; &ensp; Bagging </b>\n",
    "- The simplest way of combining predictions that \n",
    "belong to the same type.\n",
    "- Aim to decrease variance, not bias.\n",
    "- Each model receives equal weight.\n",
    "- Bagging tries to solve the over-fitting problem.<br>\n",
    "<b>Boosting</b>\n",
    "- A way of combining predictions that \n",
    "belong to the different types.\n",
    "- Aim to decrease bias, not variance.\n",
    "- Models are weighted according to their performance.\n",
    "- Boosting tries to reduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f4be3-eab1-41b6-a11b-531f004427cf",
   "metadata": {},
   "source": [
    "<b> &emsp; &ensp; Random Forest </b>\n",
    "- Ensemble of multiple decision trees\n",
    "- Less interpretable due to ensemble nature.\n",
    "- Generally performs well on large datasets.\n",
    "- Due to ensemble averaging it is less prone to overfitting. <br> <br>\n",
    "<b> Bagged Decision Tree</b> \n",
    "- Single Decision Tree\n",
    "- Highly interpretable.\n",
    "- It can perform well on small and large dataset as well.\n",
    "- More prone to overfitting specially in case of deep trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030d244-3fa8-4394-880e-71603b9a3a83",
   "metadata": {},
   "source": [
    "\n",
    "<b> &emsp; &ensp; Splitting </b>\n",
    "- Dividing a node into sub-nodes\n",
    "- It happens during tree growth\n",
    "- Maximize information gain, reduce impurity\n",
    "- Increase specificity to training data <br> <br>\n",
    "<b> Pruning</b> \n",
    "- Removing sub-nodes or branches\n",
    "- It happens after tree has been fully grown\n",
    "- Simplify the tree, improve performance on new data\n",
    "- Reduce complexity, avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafb110-4c30-43a4-a5d2-ecd893682932",
   "metadata": {},
   "source": [
    "\n",
    "<b> &emsp; &ensp; Hard Voting </b>\n",
    "- Uses discrete class predictions\n",
    "- Suitable when classifiers provide class labels only\n",
    "- Easier to implement\n",
    "- Majority rule based on class labels\n",
    "- Only the predicted class is considered\n",
    "- Simpler and faster, may be less accurate <br> <br>\n",
    "<b> Soft Voting</b> \n",
    "- Uses continuous probability estimates for each class\n",
    "- Suitable when classifiers can provide probability distributions\n",
    "- Slightly more complex due to probability handling\n",
    "- Average of predicted probabilities\n",
    "- The certainty of each prediction is considered\n",
    "- Potentially more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1dd84c",
   "metadata": {},
   "source": [
    "|Aspect|K Means clustering| K nearest neighbors|\n",
    "|--------|-----------|-----------|\n",
    "|Learning Type|Unsupervised |Supervised|\n",
    "|Purpose|Clustering data into K groups |Classification/Regression|\n",
    "|Objective|Discovering patterns in data |Predicting labels/values based on neighbors|\n",
    "|Process|Iteratively updating centroids |Storing training data and finding nearest neighbors|\n",
    "|Output|K clusters with centroids |Predicted class label/value|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c389ee8-3d1b-43b0-8054-17f89f3376ed",
   "metadata": {},
   "source": [
    "### 6. What is Information gain in Decision tree? How is it related to Entropy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34795ae2-0ec0-4648-857c-3c6159b15173",
   "metadata": {},
   "source": [
    "Information Gain is a concept used in decision tree algorithms, it measures how well an attribute separates the data into homogeneous subsets. It is closely related to entropy, as it calculates the reduction in entropy achieved by splitting the data on a particular attribute. The attribute that provides the highest information gain is chosen for splitting, helping to create a more accurate and efficient decision tree.\n",
    "\n",
    "$$\n",
    "Information\\ Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd2a02-cbd4-4dd7-93d7-a859009514d7",
   "metadata": {},
   "source": [
    "### 7. What are the disadvantages of K-means clustering over other unsupervised algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a93678-36fd-4bb0-9727-d99adb7af9af",
   "metadata": {},
   "source": [
    "Disadvantages of k-means clustering:\n",
    "- Choosing k manually.\n",
    "\n",
    "- Clustering outliers.\n",
    "\n",
    "- Being dependent on initial values.\n",
    "\n",
    "- Scaling with number of dimensions.\n",
    "\n",
    "- Clustering data of varying sizes and density.\n",
    "\n",
    "Advantages of k-means clustering:\n",
    "\n",
    "- Relatively simple to implement.\n",
    "\n",
    "- Scales to large data sets.\n",
    "\n",
    "- Guarantees convergence.\n",
    "\n",
    "- Can warm-start the positions of centroids.\n",
    "\n",
    "- Easily adapts to new examples.\n",
    "\n",
    "- Generalizes to clusters of different shapes and sizes, such as elliptical clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3521e1c-b475-45f8-b648-68e9718e8901",
   "metadata": {},
   "source": [
    "### 8. Write a python program to calculate the entropy of a dataset if the dataset, a particular feature in the dataset and a value in that feature is given to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a66c74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of dataset for feature1 = A: 0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def entropy(dataset, feature, value):\n",
    "    # Filter dataset for instances where the feature has the given value\n",
    "    filtered_data = [data for data in dataset if data[feature] == value]\n",
    "    \n",
    "    # Count the occurrences of each class in the subset\n",
    "    class_counts = Counter(data['class'] for data in filtered_data)\n",
    "    \n",
    "    # Calculate the probability of each class in the subset\n",
    "    probabilities = [class_count / len(filtered_data) for class_count in class_counts.values()]\n",
    "    \n",
    "    # Calculate entropy using the formula: -sum(p_i * log2(p_i))\n",
    "    entropy_value = -sum(probability * math.log2(probability) for probability in probabilities if probability != 0)\n",
    "    \n",
    "    return entropy_value\n",
    "\n",
    "# Example dataset (list of dictionaries)\n",
    "dataset = [\n",
    "    {'feature1': 'A', 'feature2': 'X', 'class': 'Yes'},\n",
    "    {'feature1': 'B', 'feature2': 'Y', 'class': 'No'},\n",
    "    {'feature1': 'A', 'feature2': 'Z', 'class': 'Yes'},\n",
    "    {'feature1': 'C', 'feature2': 'X', 'class': 'No'},\n",
    "    {'feature1': 'B', 'feature2': 'Z', 'class': 'Yes'},\n",
    "    {'feature1': 'A', 'feature2': 'Y', 'class': 'No'},\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "feature_name = 'feature1'\n",
    "feature_value = 'A'\n",
    "result_entropy = entropy(dataset, feature_name, feature_value)\n",
    "print(f'Entropy of dataset for {feature_name} = {feature_value}: {result_entropy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6be12",
   "metadata": {},
   "source": [
    "### 9. Do you think decision tree model can lead to over fitting? If yes, then how to handle over fitting? If no, why?\n",
    "\n",
    "Yes, decision tree models can lead to overfitting, especially when they are allowed to grow without constraints or when the dataset is noisy or contains irrelevant features. Overfitting occurs when the model captures noise or random fluctuations in the training data rather than the underlying patterns, resulting in poor generalization to unseen data.\n",
    "\n",
    "How to Handle Overfitting in Decision Trees:\n",
    "- Limiting Tree Depth (Pruning)\n",
    "\n",
    "- Minimum Samples per Leaf (Min_samples_leaf)\n",
    "\n",
    "- Minimum Samples Split (Min_samples_split)\n",
    "\n",
    "- Maximum Features (Max_features)\n",
    "\n",
    "- Maximum Depth (Max_depth)\n",
    "\n",
    "- Cross-Validation\n",
    "\n",
    "- Ensemble Methods\n",
    "\n",
    "- Feature Selection and Engineering\n",
    "\n",
    "- Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670ac89",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### How to Handle Overfitting in Decision Trees:\n",
    "\n",
    "1. **Pruning Techniques:**\n",
    "   - **Pre-Pruning:** Stop the tree from growing before it becomes too complex by setting limits on maximum depth, minimum samples per leaf, or minimum samples for split.\n",
    "   - **Post-Pruning:** Trim parts of the tree that do not contribute significantly to accuracy, using techniques like cost-complexity pruning.\n",
    "\n",
    "2. **Regularization Parameters:**\n",
    "   - Some decision tree implementations support regularization parameters that penalize complexity, similar to regularization in linear models. These parameters can help control overfitting.\n",
    "\n",
    "3. **Limiting Features:**\n",
    "   - Limit the number of features considered for each split to prevent the model from focusing too much on irrelevant or noisy features.\n",
    "   - Use techniques like feature importance ranking or random feature selection to guide the model to focus on the most informative features.\n",
    "\n",
    "4. **Ensemble Methods:**\n",
    "   - Use ensemble methods like Random Forest, Gradient Boosting, or AdaBoost, which combine multiple decision trees or use boosting techniques to reduce overfitting.\n",
    "   - Ensemble methods aggregate predictions from multiple models, reducing variance and improving generalization.\n",
    "\n",
    "5. **Cross-Validation:**\n",
    "   - Perform cross-validation during model training to assess the model's performance on unseen data. This helps detect overfitting if the model performs well on training data but poorly on validation data.\n",
    "\n",
    "6. **Feature Engineering and Selection:**\n",
    "   - Carefully select and engineer features to reduce noise and irrelevant information in the dataset, improving the model's generalization ability.\n",
    "\n",
    "7. **Early Stopping:**\n",
    "   - Stop training the model when the performance on a validation set starts to degrade, preventing the model from fitting noise in the training data.\n",
    "\n",
    "By applying these techniques and tuning hyperparameters, you can effectively handle overfitting in decision tree models and improve their ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad9a4b",
   "metadata": {},
   "source": [
    "### 10. What do you mean by Ensemble learning? What are the benefits of ensemble learning? Discuss three other ensemble learning techniques other than Random forest.\n",
    "\n",
    "Ensemble learning is an approach in which two or more models are fitted to the same data, and the predictions of each model are combined. Ensemble learning aims to achieve better performance with the ensemble of models than with any individual model.\n",
    "\n",
    "Benefits of Ensemble Learning:\n",
    "- Making more reliable predictions\n",
    "- Reducing overfitting\n",
    "- Reducing bias and variance\n",
    "- Increasing model robustness\n",
    "\n",
    "Three Ensemble Learning Techniques Other Than Random Forest:\n",
    "- Bagging: Bagging is a technique that involves training multiple instances of the same model on different subsets of the training data.\n",
    "\n",
    "- Boosting: Boosting is a machine learning method that can be used for regression and classification problems. It generates a weak prediction model at each step and adds it to the total model by weight.\n",
    "\n",
    "- Stacking: Stacking is an ensemble learning strategy that can increase the predictive performance of machine learning models by merging the predictions of many base models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc46175",
   "metadata": {},
   "source": [
    " ### 12. Explain the following clustering algorithms.\n",
    "##### DB Scan: \n",
    "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a data clustering algorithm that groups points in space based on their density. It's a non-parametric, unsupervised algorithm that can identify clusters of different shapes and sizes in large datasets that contain noise and outliers.\n",
    "- The algorithm works by picking a random core point and adding all its neighbors to a cluster. It then chooses a second core point and starts a new cluster, repeating the process until all the points are in clusters.\n",
    "\n",
    "##### K-modes clusterin: \n",
    "- K-modes clustering is an unsupervised machine learning algorithm that groups data objects into clusters based on their categorical attributes. The algorithm uses modes, or the most frequent values, to represent the clusters instead of means or medians, which is why it's called \"K-modes\".\n",
    "- The algorithm works by identifying the modes within each cluster to determine its centroid. It uses a simple matching dissimilarity measure to deal with categorical objects, and a frequency-based method to update modes during the clustering process to minimize the clustering cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc8e5b",
   "metadata": {},
   "source": [
    "### 14. Which attribute will be selected as a root node, while constructing a Decision Tree. If attribute selection measure (ASM) is Entropy.\n",
    "\n",
    " When using Entropy as the attribute selection measure, the attribute selected as the root\n",
    " node will be the one that maximizes the information gain. Information gain measures how\n",
    " much the entropy of the dataset decreases after splitting on a particular attribute. The\n",
    " attribute with the highest information gain is chosen as the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d027769",
   "metadata": {},
   "source": [
    "###  15. Which attribute will be selected as a root node, while constructing a Decision Tree. If attribute selection measure (ASM) is Gini Index.\n",
    "\n",
    "When using the Gini Index as the attribute selection measure, the attribute selected as the\n",
    " root node will be the one that minimizes the Gini impurity. The Gini impurity measures the\n",
    " probability of misclassifying a randomly chosen element if it were randomly labeled\n",
    " according to the class distribution. The attribute with the lowest Gini impurity is chosen as\n",
    " the root node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac378b",
   "metadata": {},
   "source": [
    " ### 18. Write the python code for finding closest centroid to a sample in k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def euclidean_distance(point1, point2):\n",
    "  \"\"\"Caluculates the euclidean distance between two points\"\"\"\n",
    "  return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "def closest_centroid(point, centroids):\n",
    "  \"\"\"find the closest centroid to a point\"\"\"\n",
    "  # print('point:', point)\n",
    "  distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "  return np.argmin(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23c94f",
   "metadata": {},
   "source": [
    "### 19. What are the basic assumptions in the case of the Naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704becc4",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier makes several assumptions, including:\n",
    "- Conditional independence: Features are independent of each other given the class label. This assumption is often violated in real-world situations, but it simplifies the classification problem and makes calculations easier.\n",
    "- Equal contribution: Each feature makes an equal contribution to the outcome.\n",
    "- Normal distribution: When data features are continuous, Naive Bayes assumes the values associated with each class are distributed according to a normal distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
