{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80dce208",
   "metadata": {},
   "source": [
    "## Data Science Workshop-2 (CSE 2196)\n",
    "### ASSIGNMENT-4(LOGISTIC REGRESSION, KNN)\n",
    "\n",
    "### Name: Anish Kumar\n",
    "### Regd. No: 2241011101\n",
    "### Section: 2241007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9cef1",
   "metadata": {},
   "source": [
    "###### 1. Write down the prediction function and cost function and the corresponding python code in the context for logistic regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23640acb",
   "metadata": {},
   "source": [
    "The prediction function in logistic regression is the sigmoid function of the linear combination of the input features and the model parameters.\n",
    "\n",
    "The cost function in logistic regression is the log loss function. It measures the difference between the predicted probabilities and the actual class labels. The goal is to minimize the cost function to find the optimal coefficients for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d096175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def predict(theta, X): #Theta =[w,b]\n",
    "    return sigmoid(np.dot(X, theta)) #or sigmoid(X@w+b)\n",
    "\n",
    "def cost_function(theta, X, y):\n",
    "    m = len(y)\n",
    "    h = predict(theta, X)\n",
    "    cost = -1/m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf5ab7",
   "metadata": {},
   "source": [
    "###### 2. Define types of logistic regression.\n",
    "\n",
    "Logistic regression is a statistical model used in machine learning for binary classification problem, that takes input as independent variables and produces a probability value between 0 and 1.\n",
    "\n",
    "Types of Logistic Regression\n",
    "\n",
    "Binomial: In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.\n",
    "\n",
    "Multinomial: In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as “cat”, “dogs”, or “sheep”\n",
    "\n",
    "Ordinal: In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as “low”, “Medium”, or “High”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc8098",
   "metadata": {},
   "source": [
    "###### 3. List the difference between linear regression and logistic regression.\n",
    "\n",
    "Linear Regression\n",
    " 1. Linear Regression is a supervised regression model.\n",
    " 2. In Linear Regression, we predict the value by an integer number.\n",
    " 3. It is based on the least square estimation.\n",
    " 4. Here when we plot the training datasets, a straight line can be drawn that touches maximum plots.\n",
    " 5. Linear regression is used to estimate the dependent variable in case of a change in independent variables. For example, predict the price of houses.\n",
    " 6. Linear regression assumes the normal or gaussian distribution of the dependent variable.\n",
    " \n",
    "Logistic Regression\n",
    " 1. Logistic Regression is a supervised classification model.\n",
    " 2. In Logistic Regression, we predict the value by 1 or 0.\n",
    " 3. It is based on maximum likelihood estimation.\n",
    " 4. Positive slopes result in an S-shaped curve and negative slopes result in a Z-shaped curve.\n",
    " 5. Logistic regression is used to calculate the probability of an event. For example, classify if tissue is benign or malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e343c",
   "metadata": {},
   "source": [
    "###### 4. Let you have given the following dataset, where x1, x2 are independent variable and y is dependent variable. In the context of logistic regression, find the optimized parameters after 3rd iteration. Find prediction for [1,1.5] w.r.t. the optimized parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac833e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters:  [-0.00682175  0.06503065  0.07733742]\n",
      "Prediction for [1, 1.5]:  [0.54344393]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        predictions = predict(theta, X)\n",
    "        error = predictions - y\n",
    "        gradient = np.dot(X.T, error)\n",
    "        theta -= alpha * (1/m) * gradient\n",
    "    return theta\n",
    "\n",
    "X = np.array([[0.5, 1], [1, 2], [1.5, 2.5], [2, 3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X)) # Add a column of ones for the bias term\n",
    "# Initialize theta\n",
    "theta = np.zeros(X.shape[1])\n",
    "# Set the learning rate and the number of iterations\n",
    "alpha = 0.1\n",
    "iterations = 3\n",
    "theta = gradient_descent(X, y, theta, alpha, iterations)\n",
    "print(\"Optimized parameters: \", theta)\n",
    "# Predict for [1, 1.5]\n",
    "X_new = np.array([1, 1.5]).reshape(1, -1)\n",
    "X_new = np.hstack((np.ones((X_new.shape[0], 1)), X_new)) # Add a column of ones for the bias term\n",
    "prediction = predict(theta, X_new)\n",
    "print(\"Prediction for [1, 1.5]: \", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a557f",
   "metadata": {},
   "source": [
    "###### 5. Explain K-Nearest Neighbor (KNN) algorithm.\n",
    "\n",
    "K-nearest neighbor (KNN) is a simple algorithm that stores all available cases and classifies new data or cases based on a similarity measure. It is mostly used to classify a data point based on how its neighbors are classified.\n",
    "\n",
    "A. Let k be the number of neighbours and D be the set of training samples.\n",
    "\n",
    "B. for each test sample t = (x′, y′) do\n",
    "\n",
    " 1. Compute d, the distance between t and training sample, (x, y) ∈ D\n",
    " \n",
    " 2. Sort the calculated distances d in ascending order.\n",
    " \n",
    " 3. Get the top k rows from the sorted array.\n",
    " \n",
    " 4. Get the most frequent class corresponding to these rows.\n",
    " \n",
    " 5. Set the class of the test sample to the most frequent class.\n",
    " \n",
    "C. Return the predicted class labels of the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c475c0",
   "metadata": {},
   "source": [
    "###### 6.  How do you choose the optimal k for KNN model?\n",
    "\n",
    " 1. Cross-Validation like K-fold cross-validation where the data set is divided into k subsets, and the\n",
    " holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the\n",
    " other k-1 subsets are put together to form a training set. The average error across all k trials is\n",
    " computed. The optimal choice of k is usually the one that minimizes the test error. \n",
    " \n",
    " 2. Elbow Method: Run the KNN for a range of k values (say 1 to 20) and plot test error first error will\n",
    " decrease and reach a point (inflection point) and then rise again, option value is that inflection point. \n",
    " \n",
    "3. Sometimes, domain knowledge can also help us like in classification of medical condition only closest\n",
    " observations are revelant hence we choose a small k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4fbc7",
   "metadata": {},
   "source": [
    "###### 8. If the dataset is imbalance, then can the prediction by KNN be bias? Explain with an example.\n",
    "\n",
    " Yes, if the dataset is imbalanced, the prediction by K-Nearest Neighbors (KNN) can be biased. Imbalanced\n",
    " datasets occur when one class significantly outnumbers the other class(es). In such cases, the majority\n",
    " class tends to dominate the predictions, leading to biased results.  \n",
    " \n",
    "Suppose we have a binary classification problem where we want to predict whether a bank transaction is\n",
    " fraudulent (class 1) or not (class 0). However, the dataset is highly imbalanced, with only 1% of transactions\n",
    " being fraudulent (class 1) and 99% being non-fraudulent (class 0). \n",
    " \n",
    "  Example: Class A has 900 instances, Class B has 50 with k=3 now even if new instance will be in Class B it\n",
    " can be possible that its three nearest neighbors will all be instances of Class A due to really high majority of\n",
    " Class A, hece putting the new instance in Class A as well.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd799db8",
   "metadata": {},
   "source": [
    "###### Advantages and Disadvantages of the K-NN Algorithm\n",
    "\n",
    "Advantages:\n",
    "1. No training phase; the model quickly adapts to new data.\n",
    "2. Simple and easy to understand.\n",
    "3. Versatile, suitable for both classification and regression tasks.\n",
    "4. Robust to noisy training data and effective for non-linear relationships.\n",
    "\n",
    "Disadvantages:\n",
    "1. Computationally expensive, especially with large datasets.\n",
    "2. Sensitive to the choice of distance metric and k value.\n",
    "3. Memory-intensive, as it requires storing the entire dataset for predictions.\n",
    "4. Prone to biases when dealing with imbalanced datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
